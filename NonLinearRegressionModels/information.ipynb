{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b24eec-100d-405d-bcb4-c087d7cd0a21",
   "metadata": {},
   "source": [
    "### Doğrusal Olmayan Regresyon Modelleri Karşılaştırma Tablosu\n",
    "\n",
    "| Model Adı                    | Temel Prensip                                       | Güçlü Yönleri                                                                                                                               | Zayıf Yönleri / Dikkat Edilmesi Gerekenler                                                                                                  | Ne Zaman Kullanılmalı? (Senaryolar)                                                                                                                            | Temel Özellikler / Önemli Parametreler                                                                                               | Doğrusal Olmayan İlişkileri Yakalama |\n",
    "| :--------------------------- | :-------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------- |\n",
    "| **Yapay Sinir Ağları (ANN)** | Nöronlardan oluşan katmanlı yapı, ağırlık optimizasyonu | Çok karmaşık ve yüksek boyutlu doğrusal olmayan ilişkileri modelleyebilir, örtük özellik çıkarımı yapabilir.                               | Hesaplama maliyeti yüksek, çok fazla veri gerektirir, aşırı öğrenmeye (overfitting) eğilimli, \"kara kutu\" (yorumlanabilirliği düşük), hiperparametre ayarı zor. | Çok büyük veri setleri, görüntü/ses/metin gibi yapısal olmayan veriler, karmaşıklığın yüksek olduğu ve yorumlanabilirliğin ikinci planda olduğu problemler. | Özellik ölçeklendirme şart, katman sayısı, nöron sayısı, aktivasyon fonksiyonları, optimizasyon algoritması, öğrenme oranı.            | Mükemmel                             |\n",
    "| **CART (Karar Ağacı Regresyonu)** | Veriyi reküristif olarak alt kümelere bölen tek ağaç | Anlaması ve yorumlaması kolay, görselleştirilebilir, sayısal ve kategorik veriyi işleyebilir, az veri ön işleme gerektirir (ölçeklendirme şart değil). | Aşırı öğrenmeye çok eğilimli, verideki küçük değişikliklere duyarlı (kararsız), yumuşak doğrusal ilişkileri yakalamada zayıf kalabilir.            | Hızlı bir başlangıç modeli olarak, yorumlanabilirliğin önemli olduğu durumlar, küçük-orta ölçekli veri setleri.                                            | `max_depth`, `min_samples_split`, `min_samples_leaf` gibi budama parametreleri önemli.                                                 | İyi (parçalı sabit tahminler)        |\n",
    "| **CatBoost** | Gradyan Artırma (Gradient Boosting) ağaç topluluğu    | Yüksek performans, kategorik özellikleri otomatik ve etkili işler (ordered boosting), diğer GBT'lere göre aşırı öğrenmeye daha dirençli, varsayılan parametrelerle iyi sonuçlar. | Bazı veri setlerinde LightGBM'den yavaş olabilir, XGBoost/LightGBM kadar yaygın olmasa da popülaritesi artıyor.                               | Özellikle çok sayıda kategorik özellik içeren veri setleri, yüksek doğruluk hedeflendiğinde, az hiperparametre ayarıyla hızlı sonuç istenildiğinde.          | Kategorik özellik yönetimi çok iyi, `iterations`, `learning_rate`, `depth`.                                                        | Mükemmel                             |\n",
    "| **K-En Yakın Komşu (KNN) Regresyonu** | Tahmin için en yakın komşuların ortalamasını alır      | Anlaması basit, eğitim aşaması yok (veriyi saklar), yerel örüntüleri bulmada iyi.                                                           | Tahmin aşaması hesaplama açısından pahalı (özellikle büyük verilerde), K değeri ve uzaklık metriğine çok duyarlı, özellik ölçeklendirme şart, yüksek boyutlulukta performansı düşer. | Küçük veri setleri, veride yerel bir yapı olduğunda, basit bir başlangıç modeli gerektiğinde.                                                              | Özellik ölçeklendirme şart, K değeri kritik, `weights` (uniform, distance).                                                          | İyi (yerel doğrusal olmayanlıklar) |\n",
    "| **LightGBM** | Gradyan Artırma (Gradient Boosting) ağaç topluluğu    | Çok hızlı eğitim, bellek verimliliği yüksek (histogram tabanlı), mükemmel doğruluk, büyük veri setlerini işleyebilir, GPU desteği.        | Küçük veri setlerinde dikkatli ayar yapılmazsa aşırı öğrenebilir, hiperparametrelere duyarlı olabilir.                                       | Büyük veri setleri, eğitim hızının kritik olduğu durumlar, yüksek doğruluk gereksinimleri.                                                                  | Hızlı ve verimli, `num_leaves`, `learning_rate`, `n_estimators`, `min_child_samples`. Özellikle küçük verilerde dikkatli ayar ister. | Mükemmel                             |\n",
    "| **Random Forest (Rastgele Orman) Regresyonu** | Çok sayıda karar ağacının torbalama (bagging) ile birleşimi | İyi doğruluk, tek ağaca göre aşırı öğrenmeye daha dirençli, yüksek boyutluluk ve büyük veri setlerini işleyebilir, özellik önemini verir.       | Çok fazla ağaçla hesaplama maliyeti artabilir, tek ağaç kadar yorumlanabilir değil, eğitim verisinin dışındaki değerler için tahmin yapmada zorlanır. | Genel amaçlı iyi bir algoritma, sağlam ve görece kolay ayarlanabilen bir model gerektiğinde, özellik öneminin istendiği durumlar.                        | `n_estimators`, `max_features`, `max_depth`, `min_samples_split`. Aşırı öğrenmeye tek ağaçtan daha dirençli.                          | Mükemmel                             |\n",
    "| **Destek Vektör Regresyonu (SVR)** | Veri noktalarını ayıran hiper-düzlemi ve marjini kullanır | Yüksek boyutlu uzaylarda etkili, bellek verimli (destek vektörlerini kullanır), farklı çekirdek (kernel) fonksiyonları ile esneklik sağlar. | Çok büyük veri setlerinde eğitimi yavaş olabilir, performansı çekirdek ve parametrelerine (C, gamma, epsilon) çok bağlı, özellik ölçeklendirme şart. | Yüksek boyutlu veriler, ilişkinin aşırı karmaşık olmadığı veya belirli çekirdek varsayımlarının veriye uyduğu durumlar, orta ölçekli veri setleri.         | Özellik ölçeklendirme şart, Çekirdek (`linear`, `poly`, `rbf`) ve `C`, `gamma`, `epsilon` parametreleri kritik.                        | İyi (çekirdeğe bağlı)                |\n",
    "| **XGBoost** | Gradyan Artırma (Gradient Boosting) ağaç topluluğu    | Mükemmel performans (genellikle en iyi), yüksek optimizasyon, aşırı öğrenmeyi engelleyen regülarizasyon, eksik verileri işleyebilir, GPU desteği. | Random Forest'a göre ayarlanması daha karmaşık olabilir, küçük veri setlerinde dikkatli ayar yapılmazsa aşırı öğrenebilir.                       | Yapısal/tablosal veriler için yarışmalarda ve endüstride yaygın, en yüksek doğruluk hedeflendiğinde.                                                      | Regülarizasyon (`lambda`, `alpha`), eksik veri yönetimi, `n_estimators`, `learning_rate`, `max_depth`.                                   | Mükemmel                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0db7c19-fa17-4839-a9f6-b5d580c7b43e",
   "metadata": {},
   "source": [
    "## Makine Ogrenilmesinin Otomatiklestirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101af714-ab6a-4343-a219-cef3bc802f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphanelerin import edilmesi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Veri seti oluşturma ve ayırma için\n",
    "from sklearn.datasets import make_regression # Örnek veri seti için\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # MLP, KNN, SVR için özellik ölçeklendirme\n",
    "\n",
    "# Metrikler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Regresyon Modelleri\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor # RandomForest'u da ekledim, yaygın kullanılır.\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# 1. Örnek Veri Seti Oluşturma ve Hazırlama\n",
    "# Gerçek veri setinizle bu kısmı değiştirebilirsiniz.\n",
    "X, y = make_regression(n_samples=200, n_features=19, random_state=42, noise=0.1)\n",
    "# Örnek olması açısından Pandas DataFrame'e çevirelim (opsiyonel)\n",
    "# X_df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(X.shape[1])])\n",
    "# y_series = pd.Series(y, name='target')\n",
    "\n",
    "# Veri setini eğitim ve test olarak ayırma\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# 2. Özellik Ölçeklendirme (Feature Scaling)\n",
    "# MLP, KNN ve SVR gibi bazı modeller özellik ölçeklendirmesine duyarlıdır.\n",
    "# Ağaç tabanlı modeller (Decision Tree, RandomForest, GBM, XGBoost, LightGBM) genellikle etkilenmez.\n",
    "# Ancak tüm modellere aynı veriyi vermek için burada ölçeklendirme yapabiliriz.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 3. Denenecek Modellerin Listesi\n",
    "# Her bir modeli istediğiniz varsayılan veya başlangıç parametreleriyle tanımlayabilirsiniz.\n",
    "# random_state parametresi sonuçların tekrarlanabilirliği için önemlidir.\n",
    "models_to_evaluate = [\n",
    "    (\"LGBM Regressor\", LGBMRegressor(random_state=42, verbosity=-1)), # verbosity=-1 LightGBM'in loglarını azaltır\n",
    "    (\"XGBoost Regressor\", XGBRegressor(random_state=42, objective='reg:squarederror')), # objective uyarısını engeller\n",
    "    (\"Gradient Boosting Regressor\", GradientBoostingRegressor(random_state=42)),\n",
    "    (\"Random Forest Regressor\", RandomForestRegressor(random_state=42)),\n",
    "    (\"Decision Tree Regressor\", DecisionTreeRegressor(random_state=42)),\n",
    "    (\"MLP Regressor\", MLPRegressor(random_state=42, max_iter=1000, early_stopping=False, solver='adam')), # max_iter artırıldı, early_stopping eklenebilir\n",
    "    (\"KNeighbors Regressor\", KNeighborsRegressor()),\n",
    "    (\"SVR\", SVR())\n",
    "]\n",
    "\n",
    "# 4. Modelleri Döngü İçinde Eğitme ve Değerlendirme Fonksiyonu\n",
    "def train_and_evaluate_all_models(X_train_data, y_train_data, X_test_data, y_test_data, model_list):\n",
    "    \"\"\"\n",
    "    Verilen model listesindeki tüm modelleri eğitir, tahmin yapar ve RMSE skorlarını hesaplar.\n",
    "    \"\"\"\n",
    "    model_performance = {}\n",
    "    print(\"Model Eğitimi ve Değerlendirme Süreci Başlatılıyor...\\n\" + \"=\"*40)\n",
    "\n",
    "    for name, model_instance in model_list:\n",
    "        print(f\"\\n[{name}] modeli eğitiliyor...\")\n",
    "\n",
    "        # MLP, KNN ve SVR ölçeklenmiş veri ile daha iyi çalışır. Diğerleri için fark etmeyebilir.\n",
    "        # Bu örnekte tüm modellere ölçeklenmiş veri veriyoruz, ancak isterseniz\n",
    "        # model tipine göre X_train ve X_test veya X_train_scaled ve X_test_scaled seçebilirsiniz.\n",
    "        current_X_train = X_train_data\n",
    "        current_X_test = X_test_data\n",
    "        \n",
    "        # Örnek: Modele göre veri seçimi (isterseniz bu kısmı aktif edebilirsiniz)\n",
    "        # if name in [\"MLP Regressor\", \"KNeighbors Regressor\", \"SVR\"]:\n",
    "        #     current_X_train = X_train_scaled # Ölçeklenmiş veriyi kullan\n",
    "        #     current_X_test = X_test_scaled   # Ölçeklenmiş veriyi kullan\n",
    "        # else:\n",
    "        #     current_X_train = X_train # Orijinal (veya dummy/kategorik işlem görmüş) veriyi kullan\n",
    "        #     current_X_test = X_test   # Orijinal (veya dummy/kategorik işlem görmüş) veriyi kullan\n",
    "        # Bu örnekte basitlik adına hepsine ölçeklenmiş veriyi verelim:\n",
    "        current_X_train = X_train_scaled\n",
    "        current_X_test = X_test_scaled\n",
    "        \n",
    "        try:\n",
    "            # Modeli eğitme\n",
    "            model_instance.fit(current_X_train, y_train_data)\n",
    "\n",
    "            # Test seti üzerinde tahmin yapma\n",
    "            y_pred = model_instance.predict(current_X_test)\n",
    "\n",
    "            # RMSE hesaplama\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_data, y_pred))\n",
    "            model_performance[name] = rmse\n",
    "            print(f\"[{name}] -> RMSE: {rmse:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{name}] modelinde hata oluştu: {e}\")\n",
    "            model_performance[name] = None\n",
    "        \n",
    "        print(\"-\"*30)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nTüm Modellerin Performans Özeti (RMSE):\\n\" + \"=\"*40)\n",
    "    for model_name, score in model_performance.items():\n",
    "        if score is not None:\n",
    "            print(f\"- {model_name}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"- {model_name}: Eğitilemedi veya Değerlendirilemedi.\")\n",
    "            \n",
    "    return model_performance\n",
    "\n",
    "# 5. Fonksiyonu Çağırma\n",
    "# Bu örnekte hepsine ölçeklenmiş veriyi veriyoruz.\n",
    "# Kendi veri setinizde, ağaç tabanlı modellere ölçeklenmemiş (ama kategorik özellikleri işlenmiş)\n",
    "# veriyi vermeyi tercih edebilirsiniz.\n",
    "results = train_and_evaluate_all_models(X_train_scaled, y_train, X_test_scaled, y_test, models_to_evaluate)\n",
    "\n",
    "# En iyi modeli bulma (opsiyonel)\n",
    "# Sadece başarılı bir şekilde değerlendirilen modelleri filtrele\n",
    "valid_results = {name: score for name, score in results.items() if score is not None}\n",
    "if valid_results:\n",
    "    best_model_name = min(valid_results, key=valid_results.get)\n",
    "    print(f\"\\nEn Düşük RMSE Değerine Sahip Model: {best_model_name} (RMSE: {valid_results[best_model_name]:.4f})\")\n",
    "else:\n",
    "    print(\"\\nHiçbir model başarıyla değerlendirilemedi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
